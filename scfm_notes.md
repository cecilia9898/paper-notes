## scfm.py code 
## ğŸ” ä»£ç æœ¬ä½“ï¼š

```python
def loss(self, model, guide, *args, **kwargs):
    """
    Not needed by default.
    """
    raise NotImplementedError
```

---

## ğŸ§  ä¸­æ–‡è§£é‡Šï¼š

ä½ åœ¨è¿™é‡Œ**æ•…æ„ä¸å®ç° `loss()` æ–¹æ³•**ï¼Œå¹¶ä¸”æ˜¾å¼æŠ›å‡º `NotImplementedError`ï¼Œæ„æ€æ˜¯ï¼š

> â€œé»˜è®¤çš„ loss æ¥å£æˆ‘ä¸æ‰“ç®—æ”¯æŒï¼Œæˆ‘åªæ”¯æŒ differentiable_lossã€‚â€

---

## ğŸ¤” é‚£é—®é¢˜æ¥äº†ï¼šPyro æœ‰ä¸¤ä¸ª loss æ–¹æ³•ï¼Œå•¥åŒºåˆ«ï¼Ÿ

| æ–¹æ³•å | æ˜¯å¦å¯å¯¼ï¼Ÿ | ç”¨é€” | é»˜è®¤è¿”å›å€¼ |
|--------|------------|------|------------|
| `loss()` | âŒ ä¸å¯å¯¼ï¼ˆåªè¿”å›æ ‡é‡ lossï¼‰ | ç”¨äº loggingã€early stoppingã€æ‰“å° ELBO | `float` |
| `differentiable_loss()` | âœ… å¯å¯¼ | ç”¨äºä¼˜åŒ–å™¨çš„åå‘ä¼ æ’­ | `torch.Tensor` æˆ–å¤šä¸ª tensor |

Pyro çš„ `Trace_ELBO` é»˜è®¤æ”¯æŒä¸¤ä¸ªæ¥å£ï¼š

```python
loss = elbo.loss(model, guide)                # logging ç”¨
loss_tensor = elbo.differentiable_loss(model, guide)  # è®­ç»ƒç”¨ï¼ˆbackpropï¼‰
```

---

## ğŸ¯ ä½ ä¸ºä»€ä¹ˆç¦ç”¨äº† `loss()`ï¼Ÿ

å› ä¸ºä½ è‡ªå®šä¹‰çš„ç‰ˆæœ¬**è¿”å›çš„æ˜¯ä¸€ä¸ª loss çš„ tupleï¼ˆæ¯ä¸ªå˜é‡ä¸€ä¸ªï¼‰**ï¼Œè€Œ Pyro é»˜è®¤çš„ `loss()` æœŸæœ›çš„æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œåˆ«äººå®¶çš„ `loss()` è¿”å›ï¼š

```python
-312.4
```

ä½ å®¶çš„ `differentiable_loss()` è¿”å›ï¼š

```python
(loss_latent1, loss_latent2, ..., loss_latentN)
```

è¿™ä¸¤ä¸ªæ¥å£æ ¹æœ¬ä¸å…¼å®¹ï¼ŒPyro è‡ªå·±ä¹Ÿæ²¡æ³•çŸ¥é“æ€ä¹ˆè‡ªåŠ¨åŠ å’Œã€å±•ç¤ºã€åä¼ ï¼Œæ‰€ä»¥ä½ å¹²è„†æ˜è¯´ï¼š

> ä¸ç»™ä½  `loss()`ï¼Œåˆ«è°ƒç”¨å®ƒã€‚

---

## ğŸ’¡ å¦‚æœä½ ä¸ç¦ç”¨å®ƒä¼šæ€ä¹ˆæ ·ï¼Ÿ

Pyro ä¼šåœ¨è®­ç»ƒæ—¶å¯èƒ½å°è¯•è®¿é—® `.loss()`ï¼š

```python
elbo = Trace_ELBO_site(["z1", "z2"])
elbo.loss(model, guide)
```

è¿™æ—¶å€™å°±ä¼šè¿”å›ä¸€ä¸ªé”™è¯¯çš„ç»“æœï¼ˆæˆ–è€…å‡º bugï¼‰ï¼Œå› ä¸ºä½ æœ¬æ„æ˜¯åˆ†å˜é‡è¿½è¸ª lossï¼Œä¸æ˜¯æ±‚å’Œã€‚

æ‰€ä»¥æœ€å®‰å…¨çš„åšæ³•å°±æ˜¯ï¼šæŠ›å‡º `NotImplementedError`ï¼Œå‘Šè¯‰ä½¿ç”¨è€…ä¸è¦ç¢°å®ƒã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> ä½ ç¦ç”¨äº† `loss()`ï¼Œæ˜¯å› ä¸ºä½ åªå®ç°äº†æ›´ç»†ç²’åº¦ã€æ›´çµæ´»çš„ `differentiable_loss()`ï¼Œè¿™ä¹Ÿæ˜¯ä½ è‡ªå®šä¹‰ ELBO çš„ä¸»è¦ç›®çš„â€”â€”**è¿½è¸ªæ¯ä¸ªå˜é‡çš„ lossï¼Œè€Œä¸æ˜¯åªè¦ä¸€ä¸ªæ€»å’Œæ ‡é‡**ã€‚

---
---

## ğŸ“£ ä¸€å¥è¯è§£é‡Šï¼š

> **åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰æ˜¯ä¸€ç§ç”¨é“¾å¼æ³•åˆ™ï¼ˆé“¾å¼æ±‚å¯¼ï¼‰è®¡ç®—ç¥ç»ç½‘ç»œä¸­å„ä¸ªå‚æ•°å¯¹æœ€ç»ˆ loss çš„æ¢¯åº¦çš„æ–¹æ³•ã€‚**

ç”¨æ›´ä¸­äºŒä¸€ç‚¹çš„è¯´æ³•ï¼š
> å®ƒæ˜¯ä¸€åœºä» loss å‡ºå‘ã€ç©¿è¶Šç¥ç»ç½‘ç»œæ¯å±‚ã€é€ä¸ªå‘Šè¯‰å‚æ•°è¯¥æ€ä¹ˆæ›´æ–°çš„æ—…ç¨‹ã€‚

---

## ğŸ§® æ•°å­¦ä¸Šæ˜¯å•¥ï¼Ÿ

å‡è®¾ä½ æœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè¾“å…¥ $x$ï¼Œè¾“å‡ºæ˜¯ $\hat{y}$ï¼Œç„¶åæœ‰ä¸€ä¸ª loss å‡½æ•°ï¼š

\[
L = \text{Loss}(y, \hat{y})
\]

åå‘ä¼ æ’­çš„ç›®æ ‡å°±æ˜¯ï¼š

\[
\frac{\partial L}{\partial \theta}
\]

ä¹Ÿå°±æ˜¯**æŸå¤±å‡½æ•°å¯¹å‚æ•° $\theta$ çš„æ¢¯åº¦**ï¼Œå‘Šè¯‰ä½ æ€ä¹ˆç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼ˆæˆ–å…¶ä»–ä¼˜åŒ–å™¨ï¼‰æ¥æ›´æ–° $\theta$ã€‚

---

## ğŸ” ç”¨é“¾å¼æ³•åˆ™ä¸€æ­¥æ­¥è®¡ç®—

ç¥ç»ç½‘ç»œæ˜¯ç”±å¾ˆå¤šå‡½æ•°å èµ·æ¥çš„ï¼Œæ¯”å¦‚ï¼š

\[
\hat{y} = f_3(f_2(f_1(x)))
\]

å‡è®¾ loss æ˜¯ $L(\hat{y})$ï¼Œé‚£ä½ è¦ç®—ï¼š

\[
\frac{dL}{d\theta} = \frac{dL}{d f_3} \cdot \frac{d f_3}{d f_2} \cdot \frac{d f_2}{d f_1} \cdot \frac{d f_1}{d \theta}
\]

è¿™å°±æ˜¯é“¾å¼æ³•åˆ™ï¼Œä¹Ÿå« **é“¾å¼æ±‚å¯¼ï¼ˆchain ruleï¼‰**ã€‚

---

## ğŸ”‹ åå‘ä¼ æ’­åœ¨ç¥ç»ç½‘ç»œä¸­æ€ä¹ˆç”¨ï¼Ÿ

1. **æ­£å‘ä¼ æ’­**ï¼ˆforward passï¼‰ï¼šè¾“å…¥æ•°æ® â†’ ç»è¿‡ç¥ç»ç½‘ç»œ â†’ å¾—åˆ°é¢„æµ‹å€¼ $\hat{y}$ å’Œ loss $L$

2. **åå‘ä¼ æ’­**ï¼ˆbackward passï¼‰ï¼š
   - PyTorchã€TensorFlow è‡ªåŠ¨ç”¨é“¾å¼æ³•åˆ™ä» loss å¼€å§‹å‘åä¼ æ’­
   - é€å±‚è®¡ç®—æ¢¯åº¦ $\frac{dL}{d\theta}$

3. **ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°**ï¼ˆå¦‚ Adamã€SGDï¼‰ï¼š

\[
\theta \leftarrow \theta - \eta \cdot \frac{\partial L}{\partial \theta}
\]

---

## ğŸ§  åœ¨ PyTorch ä¸­ï¼š

```python
loss = compute_loss(model(x), y)
loss.backward()    # ğŸ”¥ å¼€å§‹åå‘ä¼ æ’­ï¼
optimizer.step()   # æ›´æ–°å‚æ•°
```

PyTorch è‡ªåŠ¨ç»´æŠ¤ä¸€ä¸ª**è®¡ç®—å›¾ï¼ˆcomputation graphï¼‰**ï¼Œå®ƒä¼šè®°å½•æ¯ä¸€ä¸ªæ“ä½œï¼ˆæ¯”å¦‚åŠ æ³•ã€ä¹˜æ³•ã€éçº¿æ€§ï¼‰ï¼Œç„¶åé€šè¿‡ `backward()` æŠŠæ¢¯åº¦ä¸€å±‚ä¸€å±‚ä¼ å›å»ã€‚

---

## ğŸ¤” é‚£åœ¨æ¦‚ç‡æ¨¡å‹ / Pyro ä¸­å‘¢ï¼Ÿ

åœ¨ Pyro ä¸­çš„ `guide()` é€šå¸¸åŒ…å«å‚æ•° $\phi$ï¼Œä½ å¸Œæœ›æœ€å¤§åŒ– ELBOï¼ˆæˆ–æœ€å°åŒ–è´Ÿ ELBOï¼‰ï¼š

\[
\text{loss} = -\text{ELBO}(\phi)
\]

å½“ä½ è°ƒç”¨ï¼š

```python
loss.backward()
```

å°±ä¼šè§¦å‘ï¼š

> ELBO å¯¹ $\phi$ çš„åå‘ä¼ æ’­ï¼Œä¹Ÿå°±æ˜¯ $\frac{\partial \text{ELBO}}{\partial \phi}$

---

## ğŸ”¥ æ€»ç»“ç±»æ¯”ä¸€æ³¢ï¼š

| æœ¯è¯­ | å«ä¹‰ | ç±»æ¯” |
|------|------|------|
| æ­£å‘ä¼ æ’­ | æ•°æ®æµåŠ¨æ–¹å‘ | è¾“å…¥ â†’ è¾“å‡º |
| åå‘ä¼ æ’­ | æ¢¯åº¦å›ä¼ æ–¹å‘ | loss â†’ æ¯ä¸ªå‚æ•° |
| é“¾å¼æ³•åˆ™ | å¤šå±‚æ±‚å¯¼æ³•åˆ™ | å¤šç±³è¯ºéª¨ç‰Œ |
| `.backward()` | è°ƒç”¨åå‘ä¼ æ’­ | ç‚¹ç‡ƒå¯¼ç«ç´¢ğŸ”¥ |

---

# ğŸ’¬ ä¸€å¥è¯æ€»ç»“ï¼š

> åå‘ä¼ æ’­ = ç”¨é“¾å¼æ±‚å¯¼ä» loss å¼€å§‹ï¼Œä¸€æ­¥æ­¥æŠŠæ¢¯åº¦â€œå€’æ¨â€åˆ°æ¯ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå‘Šè¯‰ä½ è¯¥æ€ä¹ˆæ›´æ–°æ¨¡å‹ã€‚

---
---

# ğŸ’¡ ä¸€å¥è¯è§£é‡Š

> **é‡å‚æ•°åŒ–å˜é‡**ï¼šä½ å¯ä»¥æŠŠé‡‡æ ·è¿‡ç¨‹å†™æˆä¸€ä¸ªå¯å¾®çš„å‡½æ•°ï¼Œç›´æ¥å¯¹è¾“å…¥çš„å™ªå£°é‡‡æ · + å‚æ•°åšå‡½æ•°å˜æ¢ã€‚

> **éé‡å‚æ•°åŒ–å˜é‡**ï¼šæ²¡æ³•è¿™ä¹ˆå¹²ï¼åªèƒ½ç”¨ score function æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯ REINFORCE-style çš„ä¼°è®¡å™¨ï¼Œå™ªå£°å’Œå‚æ•°åˆ†ç¦»ä¸äº†ã€‚

---

# ğŸ¯ ä¸¾ä¸ªç®€å•ä¾‹å­ï¼šæ ‡å‡†æ­£æ€å˜é‡

```python
z ~ Normal(Î¼, Ïƒ)
```

ä½ å¯ä»¥é‡å‚æ•°åŒ–æˆï¼š

```python
Îµ ~ Normal(0, 1)
z = Î¼ + Ïƒ * Îµ
```

è¿™ä¸ª `z` æ˜¯ä¸€ä¸ª**é‡å‚æ•°åŒ–å˜é‡**ï¼Œå› ä¸ºå®ƒæ˜¯ Î¼ å’Œ Ïƒ çš„**å¯å¾®å‡½æ•°**ï¼Œä½ å¯ä»¥åš backpropï¼

---

# âŒ é‚£å•¥æ˜¯éé‡å‚æ•°å˜é‡ï¼Ÿ

æ¯”å¦‚ï¼š

```python
z ~ Categorical(probs=[0.2, 0.5, 0.3])
```

è¿™ä¸ª $z$ æ˜¯ç¦»æ•£å˜é‡ï¼Œä½ æ²¡æ³•å†™æˆï¼š

```
z = f(Î¸, Îµ)
```

- å®ƒçš„é‡‡æ ·è¿‡ç¨‹æ˜¯ä¸å¯å¾®çš„ï¼ˆargmax éšæœºé€‰æ‹©ï¼‰
- PyTorch è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿå®Œå…¨æ²¡æ³•å¤„ç†è¿™ä¸ªè¿‡ç¨‹

æ‰€ä»¥åªèƒ½ç”¨ Pyro çš„ score function estimatorï¼ˆaka REINFORCEï¼‰å»å¤„ç†å®ƒçš„æ¢¯åº¦ã€‚

---

# ğŸ”§ Pyro æ€ä¹ˆçŸ¥é“å˜é‡èƒ½ä¸èƒ½é‡å‚æ•°ï¼Ÿ

Pyro çš„ `pyro.sample(...)` ä¼šè‡ªåŠ¨æ£€æŸ¥ï¼š

```python
pyro.sample("z", dist.Normal(...))        âœ… é‡å‚æ•°åŒ–
pyro.sample("z", dist.Categorical(...))   âŒ éé‡å‚æ•°åŒ–
```

å¹¶åœ¨å†…éƒ¨åˆ›å»ºï¼š

- entropy term âœ…ï¼ˆå¦‚æœèƒ½é‡å‚æ•°ï¼‰
- score function term âœ…ï¼ˆå¦‚æœä¸èƒ½é‡å‚æ•°ï¼‰

ä½ åœ¨ `site["score_parts"]` ä¸­èƒ½çœ‹åˆ°è¿™ä¸¤ä¸ªã€‚

---

# ğŸ¤– Pyro æ˜¯æ€ä¹ˆå¤„ç†éé‡å‚æ•°å˜é‡çš„ï¼Ÿ

1. **é‡å‚æ•°å˜é‡**ï¼ˆå¦‚ Gaussianï¼‰ï¼š
   - ç”¨ reparameterization trick
   - ç›´æ¥ backprop è¿‡å»ï¼Œè®¡ç®—å›¾å¯å¾® âœ…

2. **éé‡å‚æ•°å˜é‡**ï¼ˆå¦‚ Categoricalã€Bernoulliï¼‰ï¼š
   - ç”¨ score function estimatorï¼ˆREINFORCEï¼‰
   - æ‰‹åŠ¨åŠ ä¸€ä¸ªä¿®æ­£é¡¹ï¼š
     \[
     \mathbb{E}_{q(z)}\left[ \log \frac{p(z)}{q(z)} \cdot \nabla \log q(z) \right]
     \]
   - è¿™æ—¶å€™å°±éœ€è¦ `log_r`ï¼

---

# ğŸ§ª æ€»ç»“å¯¹æ¯”è¡¨æ ¼

| å±æ€§ | é‡å‚æ•°åŒ–å˜é‡ | éé‡å‚æ•°åŒ–å˜é‡ |
|------|--------------|----------------|
| ä¸¾ä¾‹ | Normal, LogNormal | Categorical, Bernoulli |
| æ˜¯å¦å¯å¯¼ | âœ… æ˜¯ | âŒ å¦ |
| æ˜¯å¦èƒ½ç”¨ backprop | âœ… å¯ä»¥ | âŒ ä¸è¡Œï¼ˆéœ€è¦ REINFORCEï¼‰ |
| Pyro æ¢¯åº¦æ¥æº | entropy_term | score_function_term |
| æ˜¯å¦éœ€è¦ `log_r` | âŒ ä¸ç”¨ | âœ… å¿…é¡» |

---

# ğŸ”¥ æ€»ç»“ä¸€å¥è¯ï¼š

> **éé‡å‚æ•°å˜é‡ = æ²¡æ³•å¯¹é‡‡æ ·è¿‡ç¨‹åšåå‘ä¼ æ’­çš„å˜é‡ã€‚ä½ åªèƒ½ç”¨ score function gradient æ¥ä¼°è®¡å®ƒå¯¹ loss çš„è´¡çŒ®ï¼ˆå¹¶ä¸”ç‰¹åˆ« noisyï¼ï¼‰**

---
---
## ğŸ” ä»£ç å›é¡¾ï¼š

```python
log_r = None
```

**è¿™æ˜¯ä¸€ä¸ªåˆå§‹åŒ–å˜é‡çš„åŠ¨ä½œã€‚**

---

## ğŸ¯ ä¸­æ–‡è§£é‡Šï¼š

> "æˆ‘å…ˆä¸ç®— log_rï¼Œç­‰æˆ‘çœŸçš„ç”¨åˆ°å®ƒçš„æ—¶å€™å†ç®—ï¼Œè€Œä¸”åªç®—ä¸€æ¬¡ã€‚"

---

## ğŸ’¡ log_r æ˜¯å•¥ï¼Ÿ

å®ƒæ˜¯ Pyro æä¾›çš„ä¸€ä¸ªå†…éƒ¨å‡½æ•°è¾“å‡ºï¼š

```python
log_r = _compute_log_r(model_trace, guide_trace)
```

å®ƒçš„æ•°å­¦æ„ä¹‰æ˜¯ï¼š

\[
\log r(z) = \log \frac{p(z)}{q(z)}
\]

ä¹Ÿå°±æ˜¯**æ¨¡å‹å’Œ guide çš„ log æ¦‚ç‡æ¯”å€¼**ã€‚è¿™ä¸ªåœ¨ä½¿ç”¨ **score function gradient estimator**ï¼ˆé reparameterizable å˜é‡ï¼‰æ—¶æ˜¯å¿…é¡»çš„ã€‚

---

## ğŸ§  ä¸ºä»€ä¹ˆä¸€å¼€å§‹ä¸ç›´æ¥ç®—ï¼Ÿ

### ç†ç”±å¾ˆç®€å•ä¹Ÿå¾ˆå®ç”¨ï¼š

1. **èŠ‚çœè®¡ç®—èµ„æº**ï¼šå¦‚æœ guide ä¸­æ‰€æœ‰å˜é‡éƒ½æ˜¯å¯é‡å‚æ•°åŒ–ï¼ˆreparameterizableï¼‰ï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦ `log_r`ï¼Œæ‰€ä»¥ä¸å¿…ç™½ç®—ã€‚

2. **åªç®—ä¸€æ¬¡**ï¼šå¦‚æœ guide é‡Œæœ‰å¤šä¸ªå˜é‡éƒ½éœ€è¦ `log_r`ï¼Œä½ ä¹Ÿåªæƒ³ç®—ä¸€æ¬¡ï¼Œé¿å…é‡å¤è°ƒç”¨ `_compute_log_r(...)`ã€‚

---

## ğŸ§ª ä¸¾ä¸ªä¾‹å­ï¼š

æƒ³è±¡ä½  guide ä¸­æœ‰ä¸‰ä¸ªå˜é‡ï¼š"z1", "z2", "z3"ï¼Œå…¶ä¸­åªæœ‰ "z2" æ˜¯ discreteï¼Œä¸æ”¯æŒ reparameterizationã€‚

é‚£ä½ çš„ä»£ç è·‘èµ·æ¥å¤§æ¦‚åƒè¿™æ ·ï¼š

```python
log_r = None
for name in guide_trace.nodes:
    if needs_score_fn_estimator(name):  # e.g., name == "z2"
        if log_r is None:
            log_r = _compute_log_r(...)
        use(log_r)
```

è¿™æ ·ä½ åªåœ¨**ç¬¬ä¸€æ¬¡çœŸæ­£éœ€è¦å®ƒæ—¶æ‰è°ƒç”¨**ï¼Œè€Œä¸”ä¹‹åå¯ä»¥é‡å¤ä½¿ç”¨ã€‚

---

## ğŸš€ è¿™ä¸ªè®¾è®¡å«ä»€ä¹ˆï¼Ÿ

> **æ‡’è®¡ç®—ï¼ˆlazy computationï¼‰ + ç¼“å­˜ï¼ˆmemoizationï¼‰**ï¼

è¿™ç§æŠ€å·§ç‰¹åˆ«å¸¸è§äºï¼š

- PyTorch çš„ forward å‡½æ•°ï¼ˆæ¯”å¦‚æŸäº› layerï¼‰
- JAX / TensorFlow çš„ trace ç¼–è¯‘å™¨
- NumPy çš„ lazy slicing

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> `log_r = None` æ˜¯ä¸€ä¸ªèªæ˜çš„é¢„è®¾ï¼Œæ„æ€æ˜¯ï¼š
>
> â€œæˆ‘å¯èƒ½ä¼šéœ€è¦ log_rï¼Œä½†åªæœ‰å½“æˆ‘çœŸçš„é‡åˆ°éé‡å‚æ•°å˜é‡æ—¶ï¼Œæˆ‘æ‰å»ç®—è¿™ä¸ª log æ¯”å€¼ï¼Œè€Œä¸”åªç®—ä¸€æ¬¡ã€‚â€


## ä»£ç è§£é‡ŠELBO

@staticmethod
	def _differentiable_loss_particle(model_trace, guide_trace):		# pylint: disable=W0221
		"""
		Adapted to output a dictionary each for loss and surraget loss.
		"""
		from collections import defaultdict
		from pyro.infer.trace_elbo import is_identically_zero,torch_item,_compute_log_r
		loss=defaultdict(float)
		surrogate_loss=defaultdict(float)
		log_r = None

		# compute elbo and surrogate elbo
		for name, site in model_trace.nodes.items():
			if site["type"] == "sample":
				loss[name]=loss[name]-torch_item(site["log_prob_sum"])
				surrogate_loss[name]=surrogate_loss[name]-site["log_prob_sum"]

		for name, site in guide_trace.nodes.items():
			if site["type"] == "sample":
				_, score_function_term, entropy_term = site["score_parts"]

				loss[name]=loss[name]+torch_item(site["log_prob_sum"])

				if not is_identically_zero(entropy_term):
					surrogate_loss[name]=surrogate_loss[name]+entropy_term.sum()

				if not is_identically_zero(score_function_term):
					if log_r is None:
						log_r = _compute_log_r(model_trace, guide_trace)
					site = log_r.sum_to(site["cond_indep_stack"])
					surrogate_loss[name]=surrogate_loss[name]-(site * score_function_term).sum()
		loss=dict(loss)
		surrogate_loss=dict(surrogate_loss)
		return loss,surrogate_loss


# ğŸ§  é¦–å…ˆï¼šELBO çš„æ•´ä½“å½¢å¼

æ ‡å‡† ELBO çš„å½¢å¼æ˜¯ï¼š

\[
\text{ELBO}(q) = \mathbb{E}_{q(z)}[\log p(x, z) - \log q(z)]
\]

å¯ä»¥æ‹†æˆï¼š

\[
\text{ELBO}(q) = \underbrace{\mathbb{E}_{q(z)}[\log p(x \mid z)]}_{\text{é‡å»ºé¡¹}} - \underbrace{\text{KL}(q(z) \| p(z))}_{\text{æ­£åˆ™é¡¹}}
\]

---

## âœ… Pyro çš„ç»†åŒ–å†™æ³•ï¼ˆsite-wiseï¼‰

å‡è®¾æˆ‘ä»¬æ¨¡å‹ä¸­æœ‰å¤šä¸ª latent variableï¼š$z_1, z_2, \dots, z_n$ï¼Œé‚£ä¹ˆï¼š

\[
\text{ELBO}(q) = \sum_{i=1}^n \mathbb{E}_{q(z_i)}[\log p(z_i) - \log q(z_i)]
\quad (\text{æˆ‘ä»¬å¿½ç•¥ } \log p(x \mid z) \text{ é¡¹ï¼Œåªçœ‹ latent å˜é‡éƒ¨åˆ†})
\]

ä½ çš„ä»£ç åšçš„äº‹å°±æ˜¯ï¼šæŠŠè¿™ä¸ª ELBO æ‹†æˆæ¯ä¸ª site å•ç‹¬çš„ï¼š

\[
\text{ELBO}_i = \mathbb{E}_{q(z_i)}[\log p(z_i)] - \mathbb{E}_{q(z_i)}[\log q(z_i)]
\]

---

## âœ… Pyro å®é™…è®¡ç®—ä¸¤éƒ¨åˆ†ï¼š

### 1. æ™®é€š lossï¼ˆæ— æ¢¯åº¦çš„ï¼‰

ä»£ç ï¼š

```python
loss[name] -= torch_item(site["log_prob_sum"])   # from model
loss[name] += torch_item(site["log_prob_sum"])   # from guide
```

æ•°å­¦å½¢å¼ï¼š

\[
\text{loss}_i = -\log p(z_i) + \log q(z_i)
\]

æ³¨æ„è¿™é‡Œçš„ `log_prob_sum` å®é™…å°±æ˜¯ $\log p(z_i)$ æˆ– $\log q(z_i)$ï¼ŒPyro è‡ªåŠ¨æ±‚å’Œäº† plate ç»´åº¦ã€‚

è¿™ä¸ª loss æ˜¯ `.item()` è¿‡çš„ï¼Œæ˜¯æ™®é€šæ•°å€¼ï¼Œä¸ä¿æ¢¯åº¦ã€‚

---

### 2. Surrogate lossï¼ˆæœ‰æ¢¯åº¦çš„ï¼‰

ä»£ç ï¼š

```python
surrogate_loss[name] -= site["log_prob_sum"]   # model
surrogate_loss[name] += entropy_term.sum()     # guide (if reparam)
surrogate_loss[name] -= (log_r * score_term).sum()  # guide (if not reparam)
```

æˆ‘ä»¬å¯ä»¥æ•°å­¦ä¸Šå†™ä¸ºï¼š

#### å¦‚æœæ”¯æŒé‡å‚æ•°åŒ–ï¼ˆreparameterizableï¼‰ï¼š

\[
\text{surr\_loss}_i = -\log p(z_i) + \underbrace{\mathbb{E}_{q(z_i)}[-\log q(z_i)]}_{\text{Entropy}}
\]

ä¹Ÿå°±æ˜¯ï¼š

\[
\text{surr\_loss}_i = -\log p(z_i) + \text{Entropy}[q(z_i)]
\]

Pyro ä¼šé€šè¿‡ entropy_term.sum() æä¾›è¿™ä¸ªç†µé¡¹çš„ä¼°è®¡ã€‚

---

#### å¦‚æœä¸æ”¯æŒé‡å‚æ•°åŒ–ï¼ˆéé‡å‚æ•°å˜é‡ï¼‰ï¼š

Pyro ä½¿ç”¨ score function ä¼°è®¡ï¼š

\[
\text{surr\_loss}_i = -\log p(z_i) + \log q(z_i) + \left( \log \frac{p(z_i)}{q(z_i)} \cdot \nabla \log q(z_i) \right)
\]

é‚£ä¸€é¡¹ `log_r * score_function_term` å¯¹åº”çš„æ­£æ˜¯ï¼š

\[
\log \frac{p(z)}{q(z)} \cdot \nabla \log q(z)
\]

æ³¨æ„è¿™é‡Œä¸æ˜¯ç›´æ¥çš„ loss é¡¹ï¼Œè€Œæ˜¯å¯¹ **æ¢¯åº¦çš„ä¼°è®¡å™¨**ï¼Œæ‰€ä»¥å« surrogate lossã€‚

---

## ğŸ“¦ æ‰€ä»¥æ•´ä¸ªå‡½æ•°åšçš„äº‹æ˜¯ï¼š

å¯¹äºæ¯ä¸ª latent variable $z_i$ï¼š

- è¿”å›ï¼š
  - æ ‡é‡ lossï¼ˆæ— æ¢¯åº¦ï¼‰ï¼š$\boxed{\log q(z_i) - \log p(z_i)}$
  - å¯å¯¼ lossï¼ˆæœ‰æ¢¯åº¦ï¼‰ï¼šæ ¹æ®æ˜¯å¦æ”¯æŒé‡å‚æ•°ï¼Œç”¨ entropy æˆ– score function estimator åŠ ä¸Šå»

---

## ğŸ§  æ€»ç»“ä¸€ä¸‹å…¬å¼ç»“æ„

æœ€ç»ˆå¯¹æ¯ä¸ªå˜é‡ $z_i$ï¼Œä½ å¾—åˆ°çš„æ˜¯ï¼š

### æ— æ¢¯åº¦ lossï¼š
\[
\text{loss}[z_i] = \log q(z_i) - \log p(z_i)
\]

### æœ‰æ¢¯åº¦ surrogate_lossï¼š

- å¦‚æœæ”¯æŒé‡å‚æ•°åŒ–ï¼š

\[
\text{surrogate\_loss}[z_i] = -\log p(z_i) + \text{Entropy}[q(z_i)]
\]

- å¦‚æœä¸æ”¯æŒé‡å‚æ•°åŒ–ï¼š

\[
\text{surrogate\_loss}[z_i] = -\log p(z_i) + \log q(z_i) + \left( \log \frac{p(z_i)}{q(z_i)} \cdot \nabla \log q(z_i) \right)
\]

---

## ğŸ¯ ç”¨å›¾è¡¨ç¤ºçš„è¯åƒè¿™æ ·ï¼š

```
        z_i ~ guide
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ log q(z_i)  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     surrogate_loss[z_i]
    â”‚ entropy_term  â”‚â”€â”€â–¶  if reparam
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
    â”‚ score_fn_term â”‚â”€â”€â–¶  if not reparam
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–²
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ log p(z_i) â”‚â”€â”€â”€â”€â”€â”€â”€â†’  - log p(z_i) for both
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
